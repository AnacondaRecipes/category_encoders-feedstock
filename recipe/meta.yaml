{% set name = "category_encoders" %}
{% set version = "2.6.3" %}

package:
  name: {{ name }}
  version: {{ version }}

source:
  url: https://github.com/scikit-learn-contrib/{{ name }}/archive/refs/tags/{{ version }}.tar.gz
  sha256: efe91b537e7aa311fc3e6e7d61b553134fea4ba6b230da0d67d86ff5d5e7627e

build:
  number: 0
  skip: true  # [py<38]
  script: {{ PYTHON }} -m pip install . --no-deps --no-build-isolation -vv

requirements:
  host:
    - python
    - pip
    - wheel
    - setuptools
  run:
    - python
    - scikit-learn >=1.0.0
    - scipy >=0.20.0
    - pandas >=1.0.5
    - patsy >=0.5.1
    - statsmodels >=0.9.0
    - numpy >=1.14.0
    - importlib_resources # [py<39]

test:
  source_files:
    - tests
  requires:
    - pip
    - pytest
  imports:
    - category_encoders
  commands:
    - pip check
    - pytest -vv -k "not (pandas_index or truncated_index or test_extraValue or test_large_samples_binary)"
      # tests are skipped due to upstream numpy regression for numpy versions 1.24.3 (used for 3.8) and 1.25.0 (used for the other Python versions).
about:
  home: https://github.com/scikit-learn-contrib/category_encoders
  license: BSD-3-Clause
  license_family: BSD
  license_file: LICENSE.md
  summary: A collection sklearn transformers to encode categorical variables as numeric
  doc_url: https://contrib.scikit-learn.org/category_encoders
  dev_url: https://github.com/scikit-learn-contrib/category_encoders
  description: |-
    A set of scikit-learn-style transformers for encoding categorical variables
    into numeric with different techniques. While ordinal, one-hot, and hashing
    encoders have similar equivalents in the existing scikit-learn version, the
    transformers in this library all share a few useful properties:
    - First-class support for pandas dataframes as an input (and optionally as
      output)
    - Can explicitly configure which columns in the data are encoded by name or
      index, or infer non-numeric columns regardless of input type
    - Can drop any columns with very low variance based on training set
      optionally
    - Portability: train a transformer on data, pickle it, reuse it later and
      get the same thing out.
    - Full compatibility with sklearn pipelines, input an array-like dataset
      like any other transformer

extra:
  recipe-maintainers:
    - bollwyvl
    - nirajd
    - wdm0006
